{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90IRRTyD7wd6",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Prices Movements With sklearn\n",
        "#### An example of sklearn model building, training, saving in the ObjectStore, and loading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZk8tKlh7vXF",
        "colab_type": "text"
      },
      "source": [
        "### Import Libraries\n",
        "Let's start by importing the functionality we'll need to build the model and to split our data into training/testing sets. We also import pickle so we can store our model in ObjectStore later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dHcNlYW7zXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TMj7Yd6726F",
        "colab_type": "text"
      },
      "source": [
        "### Gather & Prepare Data\n",
        "Let's retrieve some intraday data for the SPY by making a History request."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h32MeKvA73-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qb = QuantBook()\n",
        "spy = qb.AddEquity('SPY')\n",
        "history = qb.History(qb.Securities.Keys, 360, Resolution.Daily)\n",
        "spy_hist = history.loc['SPY']\n",
        "spy_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oOmbM7d76-5",
        "colab_type": "text"
      },
      "source": [
        "We create a function that prepares our data suitable for training and testing our Model. We use 5 steps of OHLCV data to predict the closing price of the bar right after. By tying this to a function, we increase clarity, as well as reusability, especially if we were to copy it into a class in a .py file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cs_jJ2u77ZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to prepare our data for training our ML Model\n",
        "def prep_data(data, n_tsteps=5):\n",
        "    # n_tsteps is the number of time steps at and before time t we want to use\n",
        "    #   to predict the close price at time t + 1\n",
        "    \n",
        "    # this helps normalizes the data\n",
        "    df = data.pct_change()\n",
        "    \n",
        "    # drop the NaNs and infinities\n",
        "    with pd.option_context('mode.use_inf_as_na', True):\n",
        "        df = df.dropna()\n",
        "    \n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for i in range(len(df)-n_tsteps):\n",
        "        input_data = df.iloc[i:i+n_tsteps].values.flatten()\n",
        "        features.append(input_data)\n",
        "        label = df['close'].iloc[i+n_tsteps]\n",
        "        labels.append(label)\n",
        "\n",
        "    return np.array(features), np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWlDK4MV79Je",
        "colab_type": "text"
      },
      "source": [
        "### Build the Model (SVR with GridSearch Hyperparameter Optimization)\n",
        "Let's build the model using sklearn. We use a Support Vector Regressor as it works well with non-linear data. Furthermore, we optimize the hyperparameters of this model using GridSearchCV. We encourage users to experiment with different optimizable hyperparameters (e.g. kernel type) and models (e.g. Random Forests)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMZ2HfA97_FD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(X, y):\n",
        "    # note: grid parameters are typically unique to the model\n",
        "    param_grid = {'C': [.05, .1, .5, 1, 5, 10], 'epsilon': [0.001, 0.005, 0.01, 0.05, 0.1], 'gamma': ['auto', 'scale']} \n",
        "    gsc = GridSearchCV(SVR(), param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "    model = gsc.fit(X, y).best_estimator_\n",
        "    return model\n",
        "\n",
        "def build_model_simple(X, y):\n",
        "    # similar to above, but without hyperparameter optimization\n",
        "    model = SVR()\n",
        "    model.fit(X, y)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMcWok128AgJ",
        "colab_type": "text"
      },
      "source": [
        "Let's build and train our model by feeding in data prepared using the prep_data function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahyRIPSn8CCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = prep_data(spy_hist)\n",
        "\n",
        "# split the data for training and testing\n",
        "#   we need testing data to evaluate how well our model performs on new data \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "model = build_model(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NXg5J7w8C_j",
        "colab_type": "text"
      },
      "source": [
        "### Analyze Performance\n",
        "We then make predictions on the testing data set. We compare our Predicted Values with the Expected Values by plotting both to see if our Model has predictive power."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1JWsT_X8EXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat = model.predict(X_test)\n",
        "df = pd.DataFrame({'y': y_test.flatten(), 'y_hat': y_hat.flatten()})\n",
        "df.plot(title='Model Performance: predicted vs actual %change in closing price')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Neo6axvI8G5D",
        "colab_type": "text"
      },
      "source": [
        "### Save the Model to ObjectStore\n",
        "We dump the model using the pickle module and save the resulting bytes to ObjectStore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDIpax7A8ICm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_key = 'spy_model'\n",
        "\n",
        "pickled_model = pickle.dumps(model)\n",
        "qb.ObjectStore.SaveBytes(model_key, pickled_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD63cPAD8I84",
        "colab_type": "text"
      },
      "source": [
        "### Load Model from the ObjectStore\n",
        "Let's first retrieve the bytes of the model from ObjectStore. When we retrieve the bytes from ObjectStore, we need to cast it into a form useable by pickle with the bytearray() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmfa8etn8KeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if qb.ObjectStore.ContainsKey(model_key):\n",
        "    model_bytes = qb.ObjectStore.ReadBytes(model_key)\n",
        "    model_bytes = bytearray(model_bytes)\n",
        "    loaded_model = pickle.loads(model_bytes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7PFY3b88Lh0",
        "colab_type": "text"
      },
      "source": [
        "To ensure the model was successfully loaded, let's see if the model is able to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubINg8l-8MVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat = loaded_model.predict(X_test)\n",
        "df = pd.DataFrame({'y': y_test.flatten(), 'y_hat': y_hat.flatten()})\n",
        "df.plot(title='Model Performance: predicted vs actual %change in closing price')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhYM1KEN8Q9C",
        "colab_type": "text"
      },
      "source": [
        "# Appendix\n",
        "Below are some helper methods to manage the ObjectStore keys. We can use these to validate the saving and loading is successful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGjnx2VA8QD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ObjectStore_keys():\n",
        "    return [str(j).split(',')[0][1:] for _, j in enumerate(qb.ObjectStore.GetEnumerator())]\n",
        "\n",
        "def clear_ObjectStore():\n",
        "    for key in get_ObjectStore_keys():\n",
        "        qb.ObjectStore.Delete(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zoh62C0n8Vdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clear_ObjectStore()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}